{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OFFENSIVE LANGUAGE DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignoring the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warnings are ignored which may come during the running of code\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset which is tab separated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dataset by using pandas with tab as the delimiter\n",
    "dataset = pd.read_csv('offenseval-training-v1.tsv', delimiter='\\t')\n",
    "#creating a list of words which should be removed from the tweets\n",
    "mustBeRemovedList = [\"@USER\", \"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>95338</td>\n",
       "      <td>@USER Sometimes I get strong vibes from people...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>67210</td>\n",
       "      <td>Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>82921</td>\n",
       "      <td>@USER And why report this garbage.  We don't g...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>27429</td>\n",
       "      <td>@USER Pussy</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>46552</td>\n",
       "      <td>#Spanishrevenge vs. #justice #HumanRights and ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "0      86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1      90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2      16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3      62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4      43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "...      ...                                                ...       ...   \n",
       "13235  95338  @USER Sometimes I get strong vibes from people...       OFF   \n",
       "13236  67210  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...       NOT   \n",
       "13237  82921  @USER And why report this garbage.  We don't g...       OFF   \n",
       "13238  27429                                        @USER Pussy       OFF   \n",
       "13239  46552  #Spanishrevenge vs. #justice #HumanRights and ...       NOT   \n",
       "\n",
       "      subtask_b subtask_c  \n",
       "0           UNT       NaN  \n",
       "1           TIN       IND  \n",
       "2           NaN       NaN  \n",
       "3           UNT       NaN  \n",
       "4           NaN       NaN  \n",
       "...         ...       ...  \n",
       "13235       TIN       IND  \n",
       "13236       NaN       NaN  \n",
       "13237       TIN       OTH  \n",
       "13238       UNT       NaN  \n",
       "13239       NaN       NaN  \n",
       "\n",
       "[13240 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@USER', 'url']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the words in list \n",
    "mustBeRemovedList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making functions for text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the user tag\n",
    "def remove_userTag():\n",
    "    datasetwithoutUserTag = []\n",
    "    for line in dataset['tweet']:\n",
    "        finalListOfWords = []\n",
    "        tweets = []\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if word not in mustBeRemovedList:\n",
    "                finalListOfWords.append(word)\n",
    "        tweets = \" \".join(finalListOfWords)\n",
    "        datasetwithoutUserTag.append(tweets)\n",
    "    return datasetwithoutUserTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of words which are stop words\n",
    "noise_list = set(stopwords.words(\"english\"))\n",
    "# noise detection\n",
    "def remove_noise(input_text):\n",
    "    words = word_tokenize(input_text)\n",
    "    noise_free_words = list()\n",
    "    i = 0;\n",
    "    for word in words:\n",
    "        if word.lower() not in noise_list:\n",
    "            noise_free_words.append(word)\n",
    "        i += 1\n",
    "    noise_free_text = \" \".join(noise_free_words)\n",
    "    return noise_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing lemmatization\n",
    "def lemetize_words(input_text):\n",
    "    words = word_tokenize(input_text)\n",
    "    new_words = []\n",
    "    lem = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        word = lem.lemmatize(word, \"v\")\n",
    "        new_words.append(word)\n",
    "    new_text = \" \".join(new_words)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the dataset\n",
    "def cleaning():\n",
    "    corpus = []\n",
    "    datasetwithoutUserTag = remove_userTag()\n",
    "    for line in datasetwithoutUserTag:\n",
    "        review = re.sub('[^a-zA-Z]', ' ', line)\n",
    "        review = review.lower()\n",
    "        # remove non segnificant words\n",
    "        review = remove_noise(review)\n",
    "        review = lemetize_words(review)\n",
    "        corpus.append(review)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a bag of words\n",
    "def bagOfWordsCreation(corpus):\n",
    "    cv = CountVectorizer(max_features=12000)\n",
    "    bagOfWords = cv.fit_transform(corpus).toarray()\n",
    "    rowsValues = []\n",
    "    for line in dataset['subtask_a']:\n",
    "        if line == \"OFF\":\n",
    "            rowsValues.append(1)\n",
    "        else:\n",
    "            rowsValues.append(0)\n",
    "    return (bagOfWords, rowsValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the classifier\n",
    "def classifiers(classifier):\n",
    "    # fitting classifer to the training set\n",
    "    classifier_to_save = classifier.fit(bagOfWords_train, rowsValues_train)\n",
    "\n",
    "    # predict the test set resulty\n",
    "    rowsValues_pred = classifier.predict(bagOfWords_train)\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(rowsValues_train, rowsValues_pred)\n",
    "    print('confusuion matrix train before tunning\\n', cm)\n",
    "    accuracyTrain = (cm[0][0] + cm[1][1]) / len(rowsValues_train)\n",
    "\n",
    "    rowsValues_pred = classifier.predict(bagOfWords_test)\n",
    "    cm = confusion_matrix(rowsValues_test, rowsValues_pred)\n",
    "    print('confusuion matrix test before tunning\\n', cm)\n",
    "    accuracyTest = (cm[0][0] + cm[1][1]) / len(rowsValues_test)\n",
    "\n",
    "    return accuracyTrain, accuracyTest, classifier_to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the classifier as pickle file\n",
    "def save_classifier(classifier_name, classifier_s):\n",
    "    save_classifier = open(classifier_name + \".pickle\", \"wb\")\n",
    "    pickle.dump(classifier_s, save_classifier)\n",
    "    save_classifier.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the saved classifier before tuning\n",
    "def use_saved_classifierBeforeTunning(classifier_name):\n",
    "    classifier_f = open(classifier_name + \".pickle\", \"rb\")\n",
    "    classifier = pickle.load(classifier_f)\n",
    "    classifier_f.close()\n",
    "\n",
    "    # predict the test set resulty\n",
    "    rowsValues_pred = classifier.predict(bagOfWords_train)\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(rowsValues_train, rowsValues_pred)\n",
    "    print('confusuion matrix train before tunning\\n', cm)\n",
    "    accuracyTrain = (cm[0][0] + cm[1][1]) / len(rowsValues_train)\n",
    "\n",
    "    rowsValues_pred = classifier.predict(bagOfWords_test)\n",
    "    cm = confusion_matrix(rowsValues_test, rowsValues_pred)\n",
    "    print('confusuion matrix test before tunning\\n', cm)\n",
    "    accuracyTest = (cm[0][0] + cm[1][1]) / len(rowsValues_test)\n",
    "\n",
    "    return accuracyTrain, accuracyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the saved classifier after tuning\n",
    "def use_saved_classifierAfterTunning(classifier_name):\n",
    "    classifier_f = open(classifier_name + \".pickle\", \"rb\")\n",
    "    classifier = pickle.load(classifier_f)\n",
    "    classifier_f.close()\n",
    "\n",
    "    # predict the test set result\n",
    "    rowsValues_pred = classifier.predict(bagOfWords_train)\n",
    "    \n",
    "    # creating the confusion matrix\n",
    "    cm = confusion_matrix(rowsValues_train, rowsValues_pred)\n",
    "    print('confusuion matrix train after tunning\\n', cm)\n",
    "    accuracyTrain = (cm[0][0] + cm[1][1]) / len(rowsValues_train)\n",
    "\n",
    "    rowsValues_pred = classifier.predict(bagOfWords_test)\n",
    "    cm = confusion_matrix(rowsValues_test, rowsValues_pred)\n",
    "    print('confusuion matrix test after tunning\\n', cm)\n",
    "    accuracyTest = (cm[0][0] + cm[1][1]) / len(rowsValues_test)\n",
    "\n",
    "    return accuracyTrain, accuracyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the random grid\n",
    "def create_parameter_grid_LogisticRegression():\n",
    "    # Inverse of regularization strength; must be a positive float.\n",
    "    # Like in support vector machines, smaller values specify stronger regularization\n",
    "    C = [float(x) for x in pd.np.linspace(start=0.1, stop=5.0)]\n",
    "\n",
    "    # Create the random grid\n",
    "    random_grid = {'C':  C,\n",
    "                   }\n",
    "    print('random_grid')\n",
    "    pprint(random_grid)\n",
    "    return random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search_training(randomGrid,classifier):\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = classifier\n",
    "    # Random search of parameters, using 3 fold cross validation,\n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator=rf, param_distributions=randomGrid, n_iter=30, cv=3, verbose=0,\n",
    "                                   random_state=42, n_jobs=-1, refit=True)\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(bagOfWords_train, rowsValues_train)\n",
    "    print('rf_random.best_params')\n",
    "    var = rf_random.best_params_\n",
    "    print(var)\n",
    "    best_random = rf_random.best_estimator_\n",
    "    return best_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the model\n",
    "def evaluate(classifier):\n",
    "    rowsValues_pred = classifier.predict(bagOfWords_test)\n",
    "    cm = confusion_matrix(rowsValues_test, rowsValues_pred)\n",
    "    print('confusuion matrix test\\n', cm)\n",
    "    accuracyTest = (cm[0][0] + cm[1][1]) / len(rowsValues_test)\n",
    "    return accuracyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving our classifier \n",
    "def saveLogisticRegClassiferBeforeAndAfterTunning():\n",
    "    #Find accuracy of training and validation data before tunning\n",
    "    accuracyTrain, accuracyTest, classifier_s = classifiers(LogisticRegression())\n",
    "    save_classifier(\"logisticRegression\", classifier_s)\n",
    "    #tunning process\n",
    "    random_Grid_logistic = create_parameter_grid_LogisticRegression()\n",
    "    logistic_classifier_s_t = random_search_training(random_Grid_logistic,LogisticRegression())\n",
    "    save_classifier(\"LogisticRegressionTuned\", logistic_classifier_s_t)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building corpus and evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the corpus\n",
    "corpus = cleaning()\n",
    "bagOfWords, rowsValues = bagOfWordsCreation(corpus)\n",
    "# splitting data into training and testing data\n",
    "bagOfWords_train, bagOfWords_test, rowsValues_train, rowsValues_test = train_test_split(bagOfWords, rowsValues,\n",
    "                                                                                        test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regrission Classifier\n",
    "print('LogisticRegression')\n",
    "\n",
    "#saveLogisticRegClassiferBeforeAndAfterTunning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusuion matrix train before tunning\n",
      " [[6852  182]\n",
      " [ 835 2723]]\n",
      "confusuion matrix test before tunning\n",
      " [[1615  191]\n",
      " [ 419  423]]\n"
     ]
    }
   ],
   "source": [
    "#use saved before tunning classifier to predicit training and test sets\n",
    "accuracyTrain2, accuracyTest2 = use_saved_classifierBeforeTunning(\"logisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusuion matrix train after tunning\n",
      " [[6824  210]\n",
      " [1127 2431]]\n",
      "confusuion matrix test after tunning\n",
      " [[1636  170]\n",
      " [ 446  396]]\n"
     ]
    }
   ],
   "source": [
    "#use saved tuned classifier to predicit training and test sets\n",
    "accuracyTrainTuned2, accuracyTestTuned2 = use_saved_classifierAfterTunning(\"LogisticRegressionTuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for train set after saving base classifier =  0.9039841389728097\n",
      "accuracy for test set after saving base classifier =  0.7696374622356495\n",
      "accuracy for train set after saving tuned classifier =  0.8737726586102719\n",
      "accuracy for test set after saving tuned classifier  =  0.7673716012084593\n"
     ]
    }
   ],
   "source": [
    "print('accuracy for train set after saving base classifier = ', accuracyTrain2)\n",
    "print('accuracy for test set after saving base classifier = ', accuracyTest2)\n",
    "print('accuracy for train set after saving tuned classifier = ', accuracyTrainTuned2)\n",
    "print('accuracy for test set after saving tuned classifier  = ', accuracyTestTuned2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
